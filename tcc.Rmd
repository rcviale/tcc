---
title: "VOLATILITY OF THE CRYPTOCURRENCY MARKET AS A DRIVER OF RETURNS THROUGH A HIGH-FREQUENCY DATA APPROACH"
author: "Rodrigo Viale"
date: "28/09/2021"
bibliography: refs.bib  
#csl: abnt.csl
output:
  bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      cache = TRUE)
```

```{r library}
library(tidyverse)
library(knitr)
```

# INTRODUCTION {#intro}

The assertive allocation of investment portfolios is a difficult task to carry out, as it invariably involves a plurality of risks. Market risk in particular can never be completely eliminated, however, much is discussed in terms of its control and minimization, mainly through diversification, which depends on the correct mapping of correlations between assets. In a securities selection process aimed at portfolio optimization, therefore, it becomes necessary having not only assertive estimates for the covariance matrix, but also precise volatility measures.

A very common and popularized method in the literature dealing with volatility estimation is the Generalized Autoregressive Conditional Heteroskedasticity (GARCH), introduced by @Bollerslev86, which employs daily returns in order to get a measure for the variance of an asset. GARCH-type models, however, tend to overpredict volatility, as documented by @Nomikos11 when dealing with petroleum futures' time series. These models, therefore, present a slow response after a volatility shock passes, taking time for the estimates to go down.

With the growing amount, availability and manageability of data at higher frequencies (intraday) than the daily one, a richer sample covariance matrix can be estimated, leading ultimately to the construction of a matrix based on realized measures. 

Cryptocurrencies have gained notoriety over the last few years, especially because of the appreciation of Bitcoin, which had an incredible peak in 2018 that was only reached and surpassed by the end of 2020 (see Figures \@ref(fig:Rplot2) and \@ref(fig:Rplot1) for cumulative log-returns). These alternative assets are traded worldwide in various *exchanges*, private platforms that bare similarities to traditional stock and futures exchanges, like connecting buyers and sellers, but also some peculiarities, like unlimited trading 24 hours a day every day of the year, which increases the amount of data per day, since negotiation is not limited to a specific time frame of each day. From 2018 many new digital currencies were introduced, each with a different purpose. These have also had noteworthy appreciation in general, with the total market cap of 50 most valued cryptocurrencies having reached over 1,9 trillion dollars ^[Source: Coinmarketcap, in September 19, 2021]. These higher returns when compared to traditional assets, however, are attained only by the increased volatility that predominates in this market.

This study aims to verify if this additional volatility is a significant factor in determining cryptocurrencies' returns and, simultaneously, if there are benefits to using high frequency data for the covariance matrix estimation, instead of a daily one.

In Chapter \@ref(litrev), a literature review about the matter is introduced, Chapter \@ref(data) briefly describes the data set, Chapter \@ref(empmet) presents the empirical methodology applied in this work, Chapter \@ref(res) presents the obtained results and finally, Chapter \@ref(conc) concludes.

# LITERATURE REVIEW {#litrev}

# DATA DESCRIPTION {#data}

The data used in this work consists of ten minute-by-minute cryptocurrency prices series obtained from Binance's API ^[Source: https://api.binance.com/] and negotiated in this exchange, which is currently the largest one in the world in terms of daily trading volume of cryptocurrencies ^[Source: CoinMarketCap, at https://coinmarketcap.com/rankings/exchanges/]. It is also one of the world leaders in terms of number of "markets", where a "market" is defined as the pair of currencies that are being traded. 

This study's sample was restricted to the period between August 1, 2019 to July 31, 2021 (731 days in total), because not all series were available from the source before these dates. The analyzed cryptocurrencies are listed in Table \@ref(tab:restsample), along with each series' number of missing and available observations during this sample period. There is a total of 1,051,640 time stamps (minutes) during this time window, with 0.25% of it being of missing data. Additional information on the availability of the individual series, with each series' initial dates on Binance's API can be found in Appendix \@ref(adata).

```{r restsample}
path <- "C:/Users/rodri/OneDrive/Documents/Academics/Trabalho de ConclusÃ£o de Curso/tcc/Print/"
kable(readr::read_rds(paste0(path, "rest_summ_table.rds")), 
      caption = "Series' Sample Sizes")
```

In order to better clean this minute-by-minute data set, missing observations were filled with the previously available price and, subsequently, the last price from each 5-minute interval was taken, constructing ten series with this frequency, which then amount to 210,528 and 288 observations in total and per day, respectively, for each asset. This periodicity was chosen based on the empirical work from @Liu15, where the authors found it is very difficult to significantly beat 5-minute realized variance when it comes to price variation estimators constructed from high-frequency data, considering a span of five asset classes to reach this conclusion.

These 5-minute closing prices series were then used to compute realized variances, $RV_{i,\ t}$, and realized volatilities, $RVOL_{i,\ t}$ for each asset, according to the formulas that will be presented in Section \@ref(measures). Daily log-returns, $r_{i,\ t}$, consisting of the sum of all 5-minute log-returns in a day, were also taken. Summary statistics for these measures for each series can be found in Appendix \@ref(asumm). The assets' cumulative daily log-returns in the analyzed period are presented in Figure \@ref(fig:Rplot2).

```{r Rplot2, fig.cap = "Cryptocurrencies' Cumulative Log-Returns from August 1, 2019 to July 31, 2021", , fig.topcaption = TRUE}
knitr::include_graphics(paste0(path, "Rplot2.jpeg"))
```

In order to weight the assets and create a theoretical index to represent the "crypto market factor", the daily market capitalization of each cryptocurrency was obtained from CoinMarketCap. The weighting procedure will be described in Section \@ref(syntwei).

# METHODOLOGY {#empmet}

The methodology used in this study combines realized measures, which are presented in Section \@ref(measures), applied to real series, i.e. the cryptocurrencies that make up the initial data set, but also to fictitious series, that are created trough the assets' weighting techniques described in Section \@ref(syntwei), constituting the final data set. At the same time, a GARCH volatility estimate is computed, according to what is depicted in Section \@ref(garch). Lastly, with this final data set and the GARCH estimates, two Fama-MacBeth regressions, as presented in Section \@ref(fmb), are computed.

## REALIZED MEASURES {#measures}

For the purposes of this study, individual asset log returns are computed from the sum of the 5-min log returns for each cryptocurrency $i$, according to the formula below.

\begin{equation}
(\#eq:ri)
r_{i, t} = \sum_{j = 1}^{m} r_{i, t - 1 + j n}\  \text{ and }\
R_i = 
\begin{bmatrix}
r_{i,\ 1}\\
r_{i,\ 2}\\
... \\
r_{i,\ T}
\end{bmatrix}
\end{equation}

where $t$ represents a day, $n$ is a fraction of a trading session associated with the sampling frequency (since 5-min returns are being used and there are 1440 minutes in one day, $n = \frac{1}{(1440/5)} = \frac{1}{288}$), $m$ represents the amount of observations in on day ($m = \frac{1}{n} = 288$) and $T$ is the last observation day.

Following the research from Liu et al. (2015), which concluded that it is very difficult to significantly beat a simple 5-minute realized variance ($RV$) when it comes to price variation estimators constructed from high-frequency data, this realized measure is therefore taken for the analysis conducted in this study. This estimator consists of the sum of squared 5-minute returns for each day $t$ and asset $i$, as in the formula below.

\begin{equation}
(\#eq:rv)
RV_{i,\ t} = \sum_{j = 1}^{m} r_{i,\ t - 1 + j n}^{2}
\end{equation}

where $n$ is a fraction of a trading session associated with the sampling frequency (since 5-min $RV$ is being used and there are 1440 minutes in one day, $n = \frac{1}{(1440/5)} = \frac{1}{288}$) and $m$ represents the amount of observations in on day ($m = \frac{1}{n} = 288$).

The volatility $RVOL_{i,\ t}$ provided by this estimator is then simply the square root of each $RV_{i,\ t}$.

\begin{equation}
(\#eq:rvol)
RVOL_{i,\ t} = \sqrt{RV_{i, t}}
\end{equation}

## SYNTHETIC MARKET ESTIMATES AND WEIGHTING METHODS {#syntwei}

In order to create synthetic indices that are able to represent the the overall cryptocurrency market, it becomes necessary to weight the assets in order to create this index. Two methods, therefore, are employed as weighting factors: the daily market capitalization for each asset and the weights derived from the application of the Principal Component Analysis (PCA).

### ESTIMATION BASED ON MARKET CAPITALIZATION {#mcap}

From the daily market capitalization series, weights $w_{i,\ t}$ are estimated such that

\begin{equation}
(\#eq:wi)
w_{i,\ t}^{(MC)} = \frac{MC_{i, t}}{\sum_{j = 1}^K MC_{j, t}}
\end{equation}

\begin{equation}
(\#eq:wmc)
W_{MC,\ t}=
\begin{bmatrix}
w_{1,\ t} \\
w_{2,\ t} \\
... \\
w_{K,\ t} \\
\end{bmatrix}
\end{equation}

where $MC_{i,\ t}$ is the market capitalization for asset $i$ in period $t$.

These weights are then combined with the assets' returns in equation \@ref(eq:ri), computed previously to obtain an estimate of *market* returns $R_{m,\ t}^{(MC)}$ such as

\begin{equation}
(\#eq:caprm)
r_{m,\ t}^{(MC)} = \sum_{i = 1}^K (r_{i,\ t} w_{i,\ t})
\end{equation}

\begin{equation}
(\#eq:capRm)
R_{m}^{(MC)} =
\begin{bmatrix}
r_{m,\ 1}^{(MC)} \\
r_{m,\ 2}^{(MC)} \\
... \\
r_{m,\ T}^{(MC)}
\end{bmatrix} 
=
\begin{bmatrix}
\sum_{i = 1}^K (r_{i,\ 1} w_{i,\ 1}) \\
\sum_{i = 1}^K (r_{i,\ 2} w_{i,\ 2}) \\
... \\
\sum_{i = 1}^K (r_{i,\ T} w_{i,\ T})
\end{bmatrix}.
\end{equation}

This vector will be the first regressor in the Fama-MacBeth estimation.

Testing the market's volatility as a driver of returns is the main goal of this work, however, obtaining an estimate for the market's volatility is quite more tricky. The variance can not be weighted in the same fashion as what was done for assets' returns, because of the covariance between assets ^[Explain why variance can't be multiplied and summed up.], and also, there is no 5-minute market returns series (only daily), where the sum of squared returns could be taken, similarly as to what was done with the individual cryptocurrencies. To obtain an estimate for the market's 5-minute realized variance, therefore, firstly the 5-minute sample covariance matrix for each period must be estimated

\begin{equation}
(\#eq:rcovix)
RCov_{t} = 
\begin{bmatrix} 
s_{11,\ t}^2 & s_{12,\ t}^2 & ... & s_{1K,\ t}^2\\
s_{21,\ t}^2 & s_{22,\ t}^2 & ... & s_{2K,\ t}^2\\
... & ... & ... & ...\\
s_{K1,\ t}^2 & s_{K2,\ t}^2 & ... & s_{KK,\ t}^2\\
\end{bmatrix},
\end{equation}

where $s_{ij,\ t}^2$ denotes the 5-minute sample covariance between assets $i$ and $j$ in day $t$. This matrix can then be simply weighted by the $W_{MC,\ t}$ vectors obtained from the assets' market capitalization in equation \@ref(eq:wmc) in the following way.

\begin{equation}
(\#eq:caprv)
RV_{m,\ t}^{(MC)} = W_{MC,\ t}^T \times RCov_{t} \times W_{MC,\ t},
\end{equation}

where superscript $T$ denotes the transposed of the vector in question. The volatility $RVOL_M^{(MC)}$ estimate derived from market capitalization weighting is then obtained simply by taking square root of the obtained realized variance estimate, similarly to the procedure in equation \@ref(eq:rvol).

\begin{equation}
(\#eq:caprvol)
RVOL_{M}^{(MC)} = 
\begin{bmatrix}
\sqrt{RV_{M, 1}^{(MC)}}\\
\sqrt{RV_{M, 2}^{(MC)}}\\
...\\
\sqrt{RV_{M, T}^{(MC)}}\\
\end{bmatrix}
\end{equation}

### ESTIMATION BASED ON PRINCIPAL COMPONENT ANALYSIS {#pca}

An alternative method used to obtain an estimate for the market's returns and volatility is through Principal Component Analysis (PCA), introduced by @Pearson1901. In a context of closely related variables such as this data set (see the series' correlation matrix in Appendix \@ref(acor), @Brooks19 noted that PCA can be particularly useful, because it can transform the $K$ series into a new set of $K$ uncorrelated variables, by taking linear combinations of the original data set. In other terms, the PCA can be written as

$$
\begin{array}{ccccccccc}
p_{1, t} & = & \alpha_{11, t} r_{1, t} & + & \alpha_{12, t} r_{2, t} & + & ... & + & \alpha_{1K, t} r_{K, t}\\
p_{2, t} & = & \alpha_{21, t} r_{1, t} & + & \alpha_{22, t} r_{2, t} & + & ... & + & \alpha_{2K, t} r_{K, t}\\
...  & = & ... & + & ... & + & ... & + & ...\\
p_{K, t} & = & \alpha_{K1, t} r_{1, t} & + & \alpha_{K2, t} r_{2, t} & + & ... & + & \alpha_{KK, t} r_{K, t}
\end{array}
$$

where $p_{i, t}$ and $r_{i, t}$, represent, respectively, the $i$ principal component and original variable (5-minute return) in period $t$, and $\alpha_{ij, t}$ denotes the coefficient for variable $j$ in the principal component $i$ in day $t$. The sum of the squared coefficients $\alpha^2$ that were estimated must be equal to one for each individual component, as in equation \@ref(eq:condpca). The variables have to be standardized before the application of this method, although this step is not necessary in this study since it will work with log-returns and not prices' series. 

\begin{equation}
(\#eq:condpca)
\sum_{j = 1}^K \alpha_{i,\ j}^2 = 1 \ \forall \ i = 1,\ 2,\ ...,\ K.
\end{equation}

This procedure is, therefore, carried out for the 5-minute returns series for each day $t$ in the analyzed period, and from the estimated principal components, the first one, which is able to explain the largest amount of the data's variance (56% on average during the sampled period, see Appendix \@ref(apca) for additional information on PCA's application), is taken as the returns for an index to represent the cryptocurrencies market in the same fashion as the market cap weighted index constructed in Section \@ref(mcap). This first principal component can be viewed as a weighted average of the analyzed assets, so there is no need for any further calculations, providing what can be interpreted as an estimate for *market returns* $R_M^{(PCA)}$, defined as

\begin{equation}
(\#eq:pcaRm)
R_M^{(PCA)} =
\begin{bmatrix}
p_{1, 1}\\
p_{1, 2}\\
...\\
p_{1, T}
\end{bmatrix}
\end{equation}

Computing the realized volatility of the market however, requires a weighting vector to apply the same matrix multiplication as in Section \@ref(mcap). This vector is obtained from the weights attributed to each asset by the first principal component, which are the squared coefficients $\alpha_{1j, t}$ from equation \@ref(eq:condpca) for each day $t$, in the form of equation \@ref(eq:wpca). 

\begin{equation}
(\#eq:wpca)
W_{PCA,t} = 
\begin{bmatrix}
\alpha_{11, t}^2 \\
\alpha_{12, t}^2 \\
...\\
\alpha_{1K, t}^2
\end{bmatrix}
\end{equation}

The market realized variance is then obtained in the same fashion as in the market capitalization case presented in equation \@ref(eq:caprv), that is, by multiplying the transposed PCA weights vector by the 5-minute sample covariance matrix and then the PCA weights vector, as showed in equation \@ref(eq:pcarv). Lastly, the realized volatility $RVOL_{M}^{(PCA)}$ is again computed by taking the square root of the realized variance, as defined in equation \@ref(eq:rvol) and applied in equation \@ref(eq:caprvol).

\begin{equation}
(\#eq:pcarv)
RV_{M,t}^{(PCA)} = W_{PCA,t}^T \times RCov_t \times W_{PCA,t}
\end{equation}

\begin{equation}
(\#eq:pcarvol)
RVOL_M^{(PCA)} = 
\begin{bmatrix}
\sqrt{RV_{M, 1}^{(PCA)}}\\
\sqrt{RV_{M, 2}^{(PCA)}}\\
...\\
\sqrt{RV_{M, T}^{(PCA)}}\\
\end{bmatrix}
\end{equation}

With these, there are two measures to represent the cryptocurrencies market derived from PCA and two derived from market capitalization of individual assets, with one in each case being for market returns and the other one for realized volatility. 

In the next Section, a GARCH model is presented as an alternative volatility estimator.

## GARCH MODEL {#garch}

According to empirical studies from @Hansen05, where the authors compared the forecast potential of a total of 330 ARCH-type models, there is "no evidence that a GARCH(1, 1) is outperformed by more sophisticated models" in their analysis of exchange rates, by evaluating out-of-sample predictions, although this model is found inferior to the ones that can accommodate a leverage effect when analyzing IBM stock returns.  

The GARCH(p, q) process models conditional variance $\sigma_t^2$, making it possible for the variance to be dependent on its own past lags. The letters *p* and *q* determine the order of the model, with *q* representing the number of past conditional variance lags directly influencing itself in period *t* and *p*, the number of past "innovations" that directly influence the conditional variance in time *t*, where an innovation is defined as the mean-corrected return, or in the case of this study, the mean-corrected log return. As it is not the objective nor the focus of this work, GARCH of higher orders will not be presented here, only the specific case of interest, the GARCH(1, 1), but a detailed explanation can be found directly in @Bollerslev86, or in @Tsay. 

The GARCH(1, 1), therefore, as @Tsay presented, relies on past information to determine the present conditional volatility. Defining the innovations for asset *i* in period *t* as $a_{i,t} = r_{i,t} - \mu_{i,t}$, where $\mu_{i,t}$ is the mean of the returns, the model can be formalized as

\begin{equation}
(\#eq:garcha)
a_{i, t} = \sigma_{i, t} \epsilon_{i, t}
\end{equation}

\begin{equation}
(\#eq:garchs)
\sigma_{i, t}^2 = \alpha_{0, i} + \alpha_{1, i} a_{i, t - 1}^2 + \beta_{1, i} \sigma_{i, t - 1}^2,
\end{equation}

where $\epsilon_{i,t}$ is assumed to be an i.i.d. random variable following a Student's *t*-distribution, $\alpha_{0,i} > 0$, $\alpha_{1, i} \geq 0$ and $\beta_{1,i} \geq 0$. Finally, it is necessary that $\alpha_{0,i} + \beta_{1, i} < 1$, to ensure the unconditional variance of $a_{t, i}$ is finite while the conditional variance $\sigma_{i, t}^2$ is allowed from period to period. 

It can be seen that in this model a spike in $a_{i, t-1}^2$ or in $\sigma_{i, t-1}^2$ will impact $\sigma_{i, t}^2$ positively, or, in other words, large innovations or conditional variance in time *t - 1* tend to be followed by large innovations in time *t*, since an increase in $\sigma_{i, t}^2$ leads to a higher $a_{i, t}$, making it possible for volatility clusters to occur as they do in financial markets, more noticeably in crisis periods, such as the global financial crisis (2007-2008) and the more recent 2020 stock market crash caused by the COVID-19 pandemic.

For its reasonable forecasting power, allied with its simplicity and parsimony two GARCH(1, 1) models, with *t*-distributed innovations to account for heavier tails, are estimated for the market returns estimates derived from the market capitalization and from the PCA described in Sections \@ref(mcap) and \@ref(pca), respectively. In other words, two cases are computed, one where the market capitalization weighted theoretical index is modeled, providing a volatility estimate $\sigma_{MC}$ for $R_M^{(MC)}$, and in the second one, the first principal component series is subjected to a GARCH(1, 1) fitting, yielding volatility series $\sigma_{PCA}$ for $R_M^{(PCA)}$. Since these models serve only for comparison purposes, acting as benchmarks for the realized volatility measures, the obtained parameters are not focused in this work, although being presented in Appendix \@ref(agarch).

Finally, the Fama-MacBeth regression, which serves the purpose of testing rather the cryptocurrency market's volatility is a significant factor in determining assets returns, is presented in the next subsection.

## FAMA-MACBETH REGRESSION {#fmb}

@FMB introduced an approach to test the Capital Asset Pricing Model (CAPM), consisting of two steps, where the first one is running time series linear regressions, obtaining CAPM betas, and the second step is estimating out-of-sample and cross-sectional linear regressions using these betas, its squared values and the residuals' variance from the first step as regressors for assets' returns. The idea behind this study is to test not only if these different risk measures carry significance in determining out-of-sample returns, where the squared beta is also used to check for linearity. The CAPM betas represent the market factor risk and the variance of the residuals, the remaining risk not contemplated by the model (the authors refer to this as "non-$\beta$ risk"). 

Four modified versions of this procedure are estimated in this work, combining the market's realized return derived from market capitalization and from PCA with the realized volatility of the cryptocurrency market computed from these methods and also with the GARCH volatility estimates. These models are outlined below.

* **Model 1:** combines cryptocurrency market returns and realized volatility derived from the market capitalization weighting of the assets in under analysis, as described in Section \@ref(mcap). Thus, the model's regressions from the first and second step of the Fama-MacBeth approach can be written as

\begin{equation}
(\#eq:model1)
\text{First step: }\
R_{i} = \beta_{0, i} + \beta_{1, i} R_M^{(MC)} + \beta_{2, i} RVOL_i^{(MC)} + \epsilon_{i}\\

\text{Second step: }\
R_{t} = \lambda_{0, t} + \lambda_{1, t} \beta_{1} + \lambda_{2, t} \beta_{2} + u_t
\end{equation}

* **Model 2:** this version combines market returns and realized volatility obtained from the PCA technique applied to the data set in the form found in Section \@ref(pca). It can be formulated in the following way.

\begin{equation}
(\#eq:model2)
\text{First step: }\
R_{i} = \beta_{0, i} + \beta_{1, i} R_M^{(PCA)} + \beta_{2, i} RVOL_i^{(PCA)} + \epsilon_{i}\\

\text{Second step: }\
R_{t} = \lambda_{0, t} + \lambda_{1, t} \beta_{1} + \lambda_{2, t} \beta_{2} + u_t
\end{equation}

* **Model 3:** in this specification, the market returns derived from the market capitalization weighting are combined are used as an independent variable, along with the GARCH(1, 1) volatility estimate for this series that was presented in Subsection \@ref(garch).

\begin{equation}
(\#eq:model3)
\text{First step: }\
R_{i} = \beta_{0, i} + \beta_{1, i} R_M^{(MC)} + \beta_{2, i} \sigma_i^{(MC)} + \epsilon_{i}\\

\text{Second step: }\
R_{t} = \lambda_{0, t} + \lambda_{1, t} \beta_{1} + \lambda_{2, t} \beta_{2} + u_t
\end{equation}

* **Model 4:** finally, in this case, the crypto market returns representation obtained from the first component of the PCA are used, together with the GARCH(1, 1) volatility estimates for such time series, as described in Subsection \@ref(garch).

\begin{equation}
(\#eq:model4)
\text{First step: }\
R_{i} = \beta_{0, i} + \beta_{1, i} R_M^{(PCA)} + \beta_{2, i} \sigma_i^{(PCA)} + \epsilon_{i}\\

\text{Second step: }\
R_{t} = \lambda_{0, t} + \lambda_{1, t} \beta_{1} + \lambda_{2, t} \beta_{2} + u_t
\end{equation}

where $\beta_1 = (\beta_{1, 1},\ \beta_{1, 2},\ ...,\ \beta_{1, K})$, $\beta_2 = (\beta_{2, 1},\ \beta_{2, 2},\ ...,\ \beta_{2, K}$, $\lambda_1$ measures the variance of $R_t$ explained by the CAPM betas, $\lambda_2$, the coefficient of interest, gives the variance of $R_t$ explainable by the volatility measure in each model specification, and $\epsilon{i}$ and $u_t$ are normally distributed, with mean 0 and constant variance.

In each model's first step, therefore, 10 regressions are run, one for each asset $i$, while for the second step of the procedure, 731 regressions are estimated, one for each day $t$ in the sample period, obtaining 731 values for each one of the lambdas, which can then simply averaged as

\begin{equation}
(\#eq:avglambda)
\bar{\lambda_h} = \frac{\sum_{t = 1}^T \tilde{\lambda_{h, t}}}{T},\ \text{ for }\ h = 1,\ 2
\end{equation}

while its standard deviation can be computed as

\begin{equation}
(\#eq:sdlambda)
\sigma_h = \sqrt{\frac{\sum_{t = 1}^T (\tilde{\lambda_{h, t}} - \bar{\lambda_h})^2}{T - 1}},\ \text{ for }\ h = 1,\ 2.
\end{equation}

With these, a *t*-ratio $z_h$ can be computed for hypothesis testing, under the null $H_0:\ \lambda_h = 0$, as

\begin{equation}
(\#eq:tstat)
z_h = \frac{\bar{\lambda_h}\sqrt{T}}{\sigma_h}
\end{equation}

which follows a *t* distribution with $T- 1$ degrees of freedom. This hypothesis test is the final result for each model, where the inference of which betas are significant on explaining the variance of assets' returns can be done. The objective of this work is to test rather using volatility estimates derived from high-frequency data are significant explainable variables, while simultaneously checking if the GARCH(1, 1) volatility is a meaningful regressor. It is important to notice that while Fama and MacBeth performed the second step regressions, and consequently the *t*-tests, out-of-sample, that is, using the estimated betas from the one time window as independent variables for the returns of the next time span, in this work, this made only in sample, using the betas from the first as regressors for the returns in the same time-frame.

The next section describes the results from these Fama-MacBeth regressions, mainly focusing on the $\lambda_2$ coefficients' significance, responsible for the effect of the cryptocurrency market's volatility betas on individual asset returns.

# RESULTS {#res}

This Section presents the results from the estimated model specifications, focusing firstly on the *t*-tests for the obtained coefficients presented in the end of Section \@ref(fmb) and later on the residual part of the linear regressions.

## COEFFICIENTS

As a result from the estimated Fama-MacBeth regression, t-tests are performed on the $\lambda_0$, $\lambda_1$ and $\lambda_2$ estimates, with the results being presented in Table \@ref(tab:tt). From this table, it can be seen that as for the intercept ($\lambda_0$), there is no evidence supporting its significance for any of the estimated models, since the test did not reject the null hypothesis $H_0:\ \lambda_0 = 0$ at the conventional significance level $\alpha = 0.05$. 

```{r tt}
readr::read_rds(paste0(path, "fmbt_res.rds")) %>% 
  rename(Coefficient = coef, Model = model, Mean = x_mean, `Std. Error` = sd_er,
         `t-statistic` = t_stat, `p-value` = p_val) %>% 
  knitr::kable(caption = "t-tests' Results", digits = 5)
```

For the second coefficient, $\lambda_1$, which estimates the influence of the CAPM betas over assets' returns, the four model specifications were also not able to reject the null hypothesis of the t-test, leading to an assumption of insignificance in determining returns. The respective betas for each Model are presented in Table \@ref(tab:betas1), where the difference between the estimates can be seen, being attributed mostly to the distinct scaling and computation of market returns, by market capitalization or PCA, causing the specifications derived from the latter to appear significantly smaller than the ones from the former. This discrepancy in the scale of market returns' estimates is visible when computing summary statistics for both methods, which are presented in Table \@ref(tab:mktests). Market returns estimated trough PCA (Models 2 and 4, abbreviated as "M1" and "M4" in the referred table) show a much higher standard deviation than the ones from market capitalization weighting. In this context, no asset has a CAPM beta exposure higher than 1 ($beta_{1,i}$) in regards to the cryptocurrency market return, a scenario in which this specific asset should show returns with higher standard deviation than that of the market.

```{r betas1}
readr::read_rds(paste0(path, "betas1_comp.rds")) %>% 
  knitr::kable(caption = "$\\beta_{1, i}$ Coefficients' Comparison", 
               digits = 3)
```

Lastly, for the coefficient of interest in this work, $\lambda_2$, which measures the impact of the market volatility over assets' returns, only Model 1 was able to achieve estimates that can be considered statistically significant, as its p-value was approximately 0.04721, lower than the threshold of the significance level $\alpha = 0.05$, rejecting the null hypothesis of coefficients equal to zero. There is indication that these volatility betas' estimates, consequently, are priced in cryptocurrencies' returns. A unit increase (decrease) in the volatility beta, according the this model, therefore, should reflect, respectively, on a 0.00001 higher (lower) daily asset return, or, 0.001 p.p., which is equivalent to approximately 0.25 p.p. in annualized terms (using 365 days, as cryptocurrencies trade all days of the year), amounting to a small difference when put in perspective with the volatility from the assets in this class.

```{r mktests}
readr::read_rds(paste0(path, "mktest_summ.rds")) %>% 
  knitr::kable(caption = "Market Estimates Summary Statistics", digits = 5)
```

Analyzing the estimated $\beta_{2,i}$ coefficients, even for the models that yielded unsatisfactory results, presented in Table \@ref(tab:betas2), there is a considerable divergence in the estimates. Summary statistics for the different volatility measures used can be found in Table \@ref(tab:mktests), with the divergence in scaling and method again affecting the estimates for the betas. More important than this discrepancy is the presence of negative $\beta_{2,i}$ estimates in all models, and especially in Model 1, the only one in which $\lambda_2$ was significant, where 8 out 10 betas have a negative sign. This is a worrying result, as negative beta coefficients contradict intuition and the market efficiency argument in the sense that increased volatility in this case would decrease assets' returns, constituting an important deterrent for a rational agent to invest in such assets, as risk would not be rewarded.

```{r betas2}
readr::read_rds(paste0(path, "betas2_comp.rds")) %>% 
  knitr::kable(caption = "$\\beta_{2, i}$ Coefficients Comparison", digits = 3)
```

In the next subsection the residuals of the computed linear models are tested.

## RESIDUALS {#resid}

The usage of a linear regression demands the residuals to be analyzed, in order to verify possible problems that violate its assumptions, compromising the inference. The residuals in each regression of each model, therefore, are subjected to three statistical tests: first, the Ljung-Box test (@LB), to evaluate the presence of joint autocorrelation in the residuals, with a lag of 7, as it would cover a whole week of negotiations period and where $H_0:\ independently\ distributed\ data$. Second, the Breusch-Pagan test (@BP), where the null hypothesis, $H_0$, is characterized by homoskedasticity, that is, constant variance. Finally, a Jarque-Bera test (@JB), to analyze the normality of the residuals' distribution, where $H_0$ represents normally distributed data. Since in the Fama-MacBeth approach in this work there are 10 first-step linear regressions and 731 in the second-step, for each of the four models, the tests' results are presented in the form of rejection rates at the 5% significance level and separated by first and second step regressions. In other words, the amount of same-step regressions where the null hypothesis for that test was rejected in that model specification is divided by the amount of same-step regressions (or amount of tests) in that same model, as in equation \@ref(eq:rrs). The rejection rates for the first step regressions can be found in Table \@ref(tab:res1), and for the second step, in Table \@ref(tab:res2).

\begin{equation}
(\#eq:rrs)
RR_{a,b} = \frac{Amount\ of\ rejected\ tests\ a}{Number\ of\ Tests\ Performed},
\end{equation}
where $RR_{a,b}$ is the rejection rate of test $b$ (Ljung-Box, Breusch-Pagan or Jarque-Bera) for model $a$ 

```{r res1}
readr::read_rds(paste0(path, "res1.rds")) %>% 
  knitr::kable(digits = 3, caption = "Rejection Rates for Tests on the Residuals of the First Step Regressions")
```

Clearly the first step regressions (which estimate CAPM betas and betas for the volatility measures) draw concerns, as the Breusch-Pagan test indicates towards heteroskedasticity in the residuals of at least half of the regressions in all models. Autocorrelation should be assumed in twenty percent or more of the regressions' residuals depending on the model specification in question, according to the Ljung-Box tests. Lastly, for all the computed regressions in all estimated models the Jarque-Bera test is rejected, pointing towards residuals that are not normally distributed in all possible cases.

For the second step regressions, the rejection rates are presented in Table \@ref(tab:res2).

```{r res2}
readr::read_rds(paste0(path, "res2.rds")) %>% 
  knitr::kable(digits = 3, caption = "Rejection Rates for Tests on the Residuals of the Second Step Regressions")
```

The problems in the residuals are relevantly less in this step, with indication of presence of autocorrelation in the residuals in no more than 5.2% of the estimations, with Model 3 presenting only 3% of rejection in this criteria. Heteroskedasticity does not seem to be of concern in the vast majority of the regressions' residuals, with the worst case being Model 1, with a 9.6% rejection rate and in the best case, the one from Model 4, a rate of merely 0.04%. Lastly the rejection of normally distributed data occurs only in 3% of the regressions in Model 1, while for the worst performing model in this criteria, Model 2, this rate is approximately 13.4%.

There is evidence of significant problems in the residuals of the regressions in the first step, but for the second step, which is more important in the Fama-MacBeth approach, as it is when the estimated betas are tested for their effect on returns, these problems seem to be smaller damaging the inference in a smaller scale, although still relevant.

# CONCLUSION {#conc}


# BIBLIOGRAPHY {-}

<div id="refs"></div>


# (APPENDIX) Appendix {-}

# Additional Data Description

## Series' Data Availability Information {#adata}
```{r}
readr::read_rds(paste0(path, "full_summ_table.rds")) %>%
  # mutate(`Initial Date` = lubridate::as_date(`Initial Date`)) %>% 
  kable(caption = "Series' Initial Dates and Total Available Observations", digits = 4)
```

```{r Rplot1, fig.cap = "Cryptocurrencies' cumulative log-returns from September 1, 2017 to July 31, 2021", fig.topcaption = TRUE}
knitr::include_graphics(paste0(path, "Rplot1.jpeg"))
```

## Series' Summary Statistics {#asumm}

```{r retssumm}
kable(readr::read_rds(paste0(path, "tablea13.rds")), caption = "$r_{i,\ t}$ summary statistics",
      digits = 4)
```

```{r rvssumm}
kable(readr::read_rds(paste0(path, "tablea11.rds")), caption = "$RV_{i,\ t}$ summary statistics",
      digits = 4)

```

```{r rvolsumm}
kable(readr::read_rds(paste0(path, "tablea12.rds")), caption = "$RVOL_{i,\ t}$ summary statistics",
      digits = 4)
```

## Series' Correlation Matrix {#acor}
```{r}
kable(readr::read_rds(paste0(path, "full_cor.rds")), caption = "Full sample 5 minute correlation matrix",
      digits = 4)
```

## PCA Summary Statistics {#apca}

## GARCH Model Outputs {#agarch}

```{r}
mkt_ret <- readr::read_rds("Data/mkt_ret.rds")
gspec <- rugarch::ugarchspec(distribution.model = "std", mean.model = list(armaOrder = c(0, 0)),
                             variance.model = list(model = "sGARCH", garchOrder = c(1, 1)))
rugarch::ugarchfit(gspec, mkt_ret$mkt_ret)
```

```{r}
pca_mkt <- readr::read_rds("Data/pca_mkt.rds")
rugarch::ugarchfit(gspec, pca_mkt$mkt_ret)
```


Not used

\begin{equation}
(\#eq:r)
R_i = \begin{bmatrix}
r_{i,\ 1}\\
r_{i,\ 2}\\
... \\
r_{i,\ T}
\end{bmatrix}
\end{equation}


